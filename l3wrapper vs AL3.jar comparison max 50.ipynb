{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare `l3wrapper` and `AL3.jar`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/andrea/Documents/Politecnico/tirocinio/src/../\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from os.path import join\n",
    "from typing import Tuple, List\n",
    "\n",
    "import arff\n",
    "from snapshottest import TestCase\n",
    "\n",
    "from src import DEFAULT_DIR\n",
    "from src.XPLAIN_explainer import XPLAIN_explainer\n",
    "from src.dataset import Dataset\n",
    "\n",
    "\n",
    "def load_arff(f) -> Dataset:\n",
    "    a = arff.load(f)\n",
    "    dataset = Dataset(a['data'], a['attributes'])\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def import_dataset_arff(f, explain_indices: List[int],\n",
    "                        random_explain_dataset: bool) -> Tuple[Dataset, Dataset, List[str]]:\n",
    "    dataset = load_arff(f)\n",
    "\n",
    "    dataset_len = len(dataset)\n",
    "    train_indices = list(range(dataset_len))\n",
    "\n",
    "    if random_explain_dataset:\n",
    "        random.seed(1)\n",
    "        # small dataset\n",
    "        MAX_SAMPLE_COUNT = 100\n",
    "        if dataset_len < (2 * MAX_SAMPLE_COUNT):\n",
    "            samples = int(0.2 * dataset_len)\n",
    "        else:\n",
    "            samples = MAX_SAMPLE_COUNT\n",
    "\n",
    "        # Randomly pick some instances to remove from the training dataset and use in the\n",
    "        # explain dataset\n",
    "        explain_indices = list(random.sample(train_indices, samples))\n",
    "    for i in explain_indices:\n",
    "        train_indices.remove(i)\n",
    "\n",
    "    train_dataset = Dataset.from_indices(train_indices, dataset)\n",
    "    explain_dataset = Dataset.from_indices(explain_indices, dataset)\n",
    "\n",
    "    return train_dataset, explain_dataset, [str(i) for i in explain_indices]\n",
    "\n",
    "\n",
    "def import_datasets_arff(f, f_explain, explain_indices: List[int],\n",
    "                         random_explain_dataset: bool) -> Tuple[Dataset, Dataset, List[str]]:\n",
    "    train_dataset = load_arff(f)\n",
    "    explain_dataset = load_arff(f_explain)\n",
    "\n",
    "    len_explain_dataset = len(explain_dataset)\n",
    "\n",
    "    if random_explain_dataset:\n",
    "        random.seed(7)\n",
    "        explain_indices = list(random.sample(range(len_explain_dataset), 300))\n",
    "        explain_dataset = Dataset.from_indices(explain_indices, explain_dataset)\n",
    "\n",
    "    return train_dataset, explain_dataset, [str(i) for i in explain_indices]\n",
    "\n",
    "\n",
    "def get_classifier(classifier_name: str):\n",
    "    if classifier_name == \"sklearn_nb\":\n",
    "        from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "        skl_clf = MultinomialNB()\n",
    "\n",
    "        return skl_clf\n",
    "\n",
    "    elif classifier_name == \"sklearn_rf\":\n",
    "        from sklearn.ensemble import RandomForestClassifier\n",
    "        from sklearn.pipeline import make_pipeline\n",
    "        from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "        pipe = make_pipeline(OneHotEncoder(), RandomForestClassifier(random_state=42))\n",
    "        skl_clf = pipe\n",
    "\n",
    "        return skl_clf\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Classifier not available\")\n",
    "\n",
    "\n",
    "def get_explanation(dataset_name: str, classifier_name: str):\n",
    "    explain_dataset_indices = []\n",
    "    if dataset_name in [join(DEFAULT_DIR, \"datasets/adult_d.arff\"),\n",
    "                        join(DEFAULT_DIR, \"datasets/compas-scores-two-years_d.arff\")]:\n",
    "        with open(dataset_name) as f, open(dataset_name[:-5] + \"_explain.arff\") as f_explain:\n",
    "            train_dataset, explain_dataset, explain_indices = import_datasets_arff(f, f_explain,\n",
    "                                                                                   explain_dataset_indices,\n",
    "                                                                                   True)\n",
    "    else:\n",
    "        with open(dataset_name) as f:\n",
    "            train_dataset, explain_dataset, explain_indices = import_dataset_arff(\n",
    "                f, explain_dataset_indices, True)\n",
    "\n",
    "    clf = get_classifier(classifier_name).fit(train_dataset.X_numpy(),\n",
    "                                              train_dataset.Y_numpy())\n",
    "    explainer = XPLAIN_explainer(clf, train_dataset)\n",
    "\n",
    "    instance = explain_dataset.get_decoded(0)\n",
    "\n",
    "    cc = explain_dataset.class_column_name()\n",
    "    target_class_index = instance[cc]\n",
    "\n",
    "    return explainer.explain_instance(explain_dataset[0], target_class_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(join(DEFAULT_DIR, \"datasets/monks.arff\")) as monks_f:\n",
    "    monks_train, monks_explain, monks_explain_indices = import_dataset_arff(monks_f, [], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l3wrapper(instance_ix, train):\n",
    "    from l3wrapper.l3wrapper import L3Classifier\n",
    "    \n",
    "    clf = L3Classifier(min_sup=0.01, min_conf=0.50)\n",
    "    clf.fit(train.X_decoded(),\n",
    "            train.Y_decoded(),\n",
    "            column_names=train.X_decoded().columns.to_list())\n",
    "    \n",
    "    def get_rule_attrs_and_values(r, clf):\n",
    "            r_attr_ixs_and_values = sorted([clf._item_id_to_item[i] for i in r.item_ids])\n",
    "            r_attrs_and_values = [(clf._column_id_to_name[c], v) for c, v in r_attr_ixs_and_values]\n",
    "            return r_attrs_and_values\n",
    "\n",
    "\n",
    "    # Perform matching: remove all rules thta use an attibute value not present in the instance to\n",
    "    # explain\n",
    "    \n",
    "    # For each rule\n",
    "    # Remove rules that use item_ids greater that instance attributes\n",
    "    rules = [list(sorted(r.item_ids)) for r in clf.lvl1_rules_ if\n",
    "                 all(i < (len(train.X_decoded().iloc[0]) + 1) for i in r.item_ids)]\n",
    "\n",
    "    return rules\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess  \n",
    "import os\n",
    "from shutil import rmtree\n",
    "import tempfile\n",
    "\n",
    "def AL3(instance_ix, train):\n",
    "    with tempfile.TemporaryDirectory() as d:\n",
    "        with open(join(d, 'Knnres.arff'), \"w\") as knnres:\n",
    "            arff.dump(\n",
    "                train.to_arff_obj(),\n",
    "                knnres)\n",
    "\n",
    "        with open(join(d, 'Filetest.arff'), \"w\") as filetest:\n",
    "            arff.dump(\n",
    "                Dataset(\n",
    "                    [train.inverse_transform_instance(train[instance_ix])],\n",
    "                    train.columns\n",
    "                ).to_arff_obj(),\n",
    "                filetest)\n",
    "\n",
    "        subprocess.call(['java', '-jar', 'AL3.jar',\n",
    "                         '-no-cv',\n",
    "                         '-t', knnres.name,\n",
    "                         '-T', filetest.name,\n",
    "                         '-S', '1.0', # minimum support\n",
    "                         '-C', '50.0', # minimum confidence\n",
    "                         '-PN', d, # temporary files folder\n",
    "                         '-SP', '10', # measure threshold\n",
    "                         '-NRUL','1']) #  maximum number of rules to classify a transaction\n",
    "\n",
    "        with open(join(d, 'impo_rules.txt'), \"r\") as impo_rules:\n",
    "            rules = impo_rules.readlines()\n",
    "\n",
    "        def parse_rules(rules_lines):\n",
    "            union_rule = []\n",
    "            rules = []\n",
    "\n",
    "            for rule_line in rules_lines:\n",
    "                rule = []\n",
    "\n",
    "                for attribute_str in rule_line.split(\",\"):\n",
    "                    attribute = int(attribute_str)\n",
    "                    rule.append(attribute)\n",
    "                    union_rule.append(attribute)\n",
    "\n",
    "                rules.append(rule)\n",
    "\n",
    "            # Remove duplicates\n",
    "            union_rule = list(sorted(set(union_rule)))\n",
    "\n",
    "            if union_rule not in rules:\n",
    "                rules.append(union_rule)\n",
    "\n",
    "            return rules\n",
    "\n",
    "        return parse_rules(rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[[1, 2]]\n",
      "[[1, 2]]\n",
      "1\n",
      "[[1, 2]]\n",
      "[[1, 2]]\n",
      "2\n",
      "[[1, 2]]\n",
      "[[1, 2]]\n",
      "3\n",
      "[[1, 2]]\n",
      "[[1, 2]]\n",
      "4\n",
      "[[1, 2]]\n",
      "[[1, 2]]\n",
      "5\n",
      "[[1, 2]]\n",
      "[[1, 2]]\n",
      "6\n",
      "[[1, 2]]\n",
      "[[1, 2]]\n",
      "7\n",
      "[[1, 2]]\n",
      "[[1, 2]]\n",
      "8\n",
      "[[1, 2]]\n",
      "[[1, 2]]\n",
      "9\n",
      "[[1, 2]]\n",
      "[[1, 2]]\n",
      "10\n",
      "[[1, 2]]\n",
      "[[1, 2]]\n",
      "11\n",
      "[[1, 2]]\n",
      "[[1, 2]]\n",
      "12\n",
      "[[1, 2]]\n",
      "[[1, 2]]\n",
      "13\n",
      "[[1, 2]]\n",
      "[[1, 2]]\n",
      "14\n",
      "[[1, 2]]\n",
      "[[1, 2]]\n",
      "15\n",
      "[[1, 2]]\n",
      "[[1, 2]]\n",
      "16\n",
      "[[1, 2]]\n",
      "[[1, 2]]\n",
      "17\n",
      "[[1, 2]]\n",
      "[[1, 2]]\n",
      "18\n",
      "[[1, 2]]\n",
      "[[1, 2]]\n",
      "19\n",
      "[[1, 2]]\n",
      "[[1, 2]]\n",
      "20\n",
      "[[1, 2]]\n",
      "[[1, 2]]\n",
      "21\n",
      "[[1, 2]]\n",
      "[[1, 2]]\n",
      "22\n",
      "[[1, 2]]\n",
      "[[1, 2]]\n",
      "23\n",
      "[[1, 2]]\n",
      "[[1, 2]]\n",
      "24\n",
      "[[1, 2]]\n",
      "[[1, 2]]\n",
      "25\n",
      "[[1, 2]]\n",
      "[[1, 2]]\n",
      "26\n",
      "[[1, 2]]\n",
      "[[1, 2]]\n",
      "27\n",
      "[[1, 2]]\n",
      "[[1, 2]]\n",
      "28\n",
      "[[1, 2]]\n",
      "[[1, 2]]\n",
      "29\n",
      "[[1, 2]]\n",
      "[[1, 2]]\n",
      "30\n",
      "[[1, 2]]\n",
      "[[1, 2]]\n",
      "31\n",
      "[[1, 2]]\n",
      "[[1, 2]]\n",
      "32\n",
      "[[1, 2]]\n",
      "[[1, 2]]\n",
      "33\n",
      "[[1, 2]]\n",
      "[[1, 2]]\n",
      "34\n",
      "[[1, 2]]\n",
      "[[1, 2]]\n",
      "35\n",
      "[[1, 2]]\n",
      "[[1, 2]]\n",
      "36\n",
      "[[1, 2]]\n",
      "[[1, 2]]\n",
      "37\n",
      "[[1, 2]]\n",
      "[[1, 2]]\n",
      "38\n",
      "[[1, 2]]\n",
      "[[1, 2]]\n",
      "39\n",
      "[[1, 2]]\n",
      "[[1, 2]]\n",
      "40\n",
      "[[1, 2]]\n",
      "[[1, 2]]\n",
      "41\n",
      "[[1, 2]]\n",
      "[[1, 2]]\n",
      "42\n",
      "[[1, 2]]\n",
      "[[1, 2]]\n",
      "43\n",
      "[[1, 2]]\n",
      "[[1, 2]]\n",
      "44\n",
      "[[1, 2]]\n",
      "[[1, 2]]\n",
      "45\n",
      "[[1, 2]]\n",
      "[[1, 2]]\n",
      "46\n",
      "[[1, 2]]\n",
      "[[1, 2]]\n",
      "47\n",
      "[[1, 2]]\n",
      "[[1, 2]]\n",
      "48\n",
      "[[1, 2]]\n",
      "[[1, 2]]\n",
      "49\n",
      "[[1, 2]]\n",
      "[[1, 2]]\n"
     ]
    }
   ],
   "source": [
    "for i in range(min(len(monks_train),50)):\n",
    "    print(i)\n",
    "    a = AL3(i, monks_train)\n",
    "    b = l3wrapper(i, monks_train)\n",
    "    assert(a == b)\n",
    "    print(a)\n",
    "    print(b)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zoo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(join(DEFAULT_DIR, \"datasets/zoo.arff\")) as zoo_f:\n",
    "    zoo_train, zoo_explain, zoo_explain_indices = import_dataset_arff(zoo_f, [], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "1\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "2\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "3\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "4\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "5\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "6\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "7\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "8\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "9\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "10\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "11\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "12\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "13\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "14\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "15\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "16\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "17\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "18\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "19\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "20\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "21\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "22\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "23\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "24\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "25\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "26\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "27\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "28\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "29\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "30\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "31\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "32\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "33\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "34\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "35\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "36\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "37\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "38\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "39\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "40\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "41\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "42\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "43\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "44\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "45\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "46\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "47\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "48\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "49\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n"
     ]
    }
   ],
   "source": [
    "for i in range(min(len(zoo_train), 50)):\n",
    "    print(i)\n",
    "    a = AL3(i, zoo_train)\n",
    "    b = l3wrapper(i, zoo_train)\n",
    "    assert(a == b)\n",
    "    print(a)\n",
    "    print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(join(DEFAULT_DIR, \"datasets/adult_d.arff\")) as adult_f:\n",
    "    with open(join(DEFAULT_DIR, \"datasets/adult_d_explain.arff\")) as adult_explain_f:\n",
    "        adult_train, adult_explain, adult_explain_indices = import_datasets_arff(adult_f, adult_explain_f, [], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[[1, 2, 5, 8], [1, 5, 8, 9], [2, 5, 8, 9], [2, 5, 8], [5, 8, 9], [8, 9, 10], [1, 2, 5, 8, 9, 10]]\n",
      "[[2, 5, 8, 10, 11], [4, 5, 8, 10, 11], [1, 5, 8, 10, 11], [5, 8, 10, 11], [1, 4, 5, 8, 9, 10], [5, 7, 8, 11], [1, 4, 5, 8, 10], [1, 2, 4, 7, 8, 9, 10], [4, 5, 8, 9, 10], [1, 7, 8, 9, 10], [1, 5, 7, 8], [1, 7, 8, 10], [2, 7, 8, 11], [1, 2, 5, 8, 9], [1, 2, 5, 8], [1, 5, 8, 9, 11], [1, 5, 8, 9], [1, 2, 7, 8, 9], [2, 5, 8, 9], [1, 2, 8, 9, 10, 11], [2, 5, 8], [5, 8, 9], [1, 7, 8, 9], [7, 8, 9], [8, 9, 10, 11], [1, 8, 10], [8, 9, 10]]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-5e99c2ab42ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ml3wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madult_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(min(len(adult_train), 50)):\n",
    "    print(i)\n",
    "    a = AL3(i, adult_train)\n",
    "    print(a)\n",
    "    b = l3wrapper(i, adult_train)\n",
    "    print(b)\n",
    "    assert(a == b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "299px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 752.85,
   "position": {
    "height": "783.85px",
    "left": "1531px",
    "right": "20px",
    "top": "117px",
    "width": "374px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
