{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare `l3wrapper` and `AL3.jar`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/andrea/Documents/Politecnico/tirocinio/src/../\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from os.path import join\n",
    "from typing import Tuple, List\n",
    "\n",
    "import arff\n",
    "from snapshottest import TestCase\n",
    "\n",
    "from src import DEFAULT_DIR\n",
    "from src.XPLAIN_explainer import XPLAIN_explainer\n",
    "from src.dataset import Dataset\n",
    "\n",
    "\n",
    "def load_arff(f) -> Dataset:\n",
    "    a = arff.load(f)\n",
    "    dataset = Dataset(a['data'], a['attributes'])\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def import_dataset_arff(f, explain_indices: List[int],\n",
    "                        random_explain_dataset: bool) -> Tuple[Dataset, Dataset, List[str]]:\n",
    "    dataset = load_arff(f)\n",
    "\n",
    "    dataset_len = len(dataset)\n",
    "    train_indices = list(range(dataset_len))\n",
    "\n",
    "    if random_explain_dataset:\n",
    "        random.seed(1)\n",
    "        # small dataset\n",
    "        MAX_SAMPLE_COUNT = 100\n",
    "        if dataset_len < (2 * MAX_SAMPLE_COUNT):\n",
    "            samples = int(0.2 * dataset_len)\n",
    "        else:\n",
    "            samples = MAX_SAMPLE_COUNT\n",
    "\n",
    "        # Randomly pick some instances to remove from the training dataset and use in the\n",
    "        # explain dataset\n",
    "        explain_indices = list(random.sample(train_indices, samples))\n",
    "    for i in explain_indices:\n",
    "        train_indices.remove(i)\n",
    "\n",
    "    train_dataset = Dataset.from_indices(train_indices, dataset)\n",
    "    explain_dataset = Dataset.from_indices(explain_indices, dataset)\n",
    "\n",
    "    return train_dataset, explain_dataset, [str(i) for i in explain_indices]\n",
    "\n",
    "\n",
    "def import_datasets_arff(f, f_explain, explain_indices: List[int],\n",
    "                         random_explain_dataset: bool) -> Tuple[Dataset, Dataset, List[str]]:\n",
    "    train_dataset = load_arff(f)\n",
    "    explain_dataset = load_arff(f_explain)\n",
    "\n",
    "    len_explain_dataset = len(explain_dataset)\n",
    "\n",
    "    if random_explain_dataset:\n",
    "        random.seed(7)\n",
    "        explain_indices = list(random.sample(range(len_explain_dataset), 300))\n",
    "        explain_dataset = Dataset.from_indices(explain_indices, explain_dataset)\n",
    "\n",
    "    return train_dataset, explain_dataset, [str(i) for i in explain_indices]\n",
    "\n",
    "\n",
    "def get_classifier(classifier_name: str):\n",
    "    if classifier_name == \"sklearn_nb\":\n",
    "        from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "        skl_clf = MultinomialNB()\n",
    "\n",
    "        return skl_clf\n",
    "\n",
    "    elif classifier_name == \"sklearn_rf\":\n",
    "        from sklearn.ensemble import RandomForestClassifier\n",
    "        from sklearn.pipeline import make_pipeline\n",
    "        from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "        pipe = make_pipeline(OneHotEncoder(), RandomForestClassifier(random_state=42))\n",
    "        skl_clf = pipe\n",
    "\n",
    "        return skl_clf\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Classifier not available\")\n",
    "\n",
    "\n",
    "def get_explanation(dataset_name: str, classifier_name: str):\n",
    "    explain_dataset_indices = []\n",
    "    if dataset_name in [join(DEFAULT_DIR, \"datasets/adult_d.arff\"),\n",
    "                        join(DEFAULT_DIR, \"datasets/compas-scores-two-years_d.arff\")]:\n",
    "        with open(dataset_name) as f, open(dataset_name[:-5] + \"_explain.arff\") as f_explain:\n",
    "            train_dataset, explain_dataset, explain_indices = import_datasets_arff(f, f_explain,\n",
    "                                                                                   explain_dataset_indices,\n",
    "                                                                                   True)\n",
    "    else:\n",
    "        with open(dataset_name) as f:\n",
    "            train_dataset, explain_dataset, explain_indices = import_dataset_arff(\n",
    "                f, explain_dataset_indices, True)\n",
    "\n",
    "    clf = get_classifier(classifier_name).fit(train_dataset.X_numpy(),\n",
    "                                              train_dataset.Y_numpy())\n",
    "    explainer = XPLAIN_explainer(clf, train_dataset)\n",
    "\n",
    "    instance = explain_dataset.get_decoded(0)\n",
    "\n",
    "    cc = explain_dataset.class_column_name()\n",
    "    target_class_index = instance[cc]\n",
    "\n",
    "    return explainer.explain_instance(explain_dataset[0], target_class_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l3wrapper(instance_ix, train):\n",
    "    from l3wrapper.l3wrapper import L3Classifier\n",
    "    \n",
    "    clf = L3Classifier(min_sup=0.01, min_conf=0.50)\n",
    "    clf.fit(train.X_decoded(),\n",
    "            train.Y_decoded(),\n",
    "            column_names=train.X_decoded().columns.to_list())\n",
    "    \n",
    "    decoded_instance = train.inverse_transform_instance(train[instance_ix])\n",
    "    encoded_rules = clf.lvl1_rules_\n",
    "\n",
    "    def decode_rule(r, clf):\n",
    "        r_class = clf._class_dict[r.class_id]\n",
    "        r_attr_ixs_and_values = sorted([clf._item_id_to_item[i] for i in r.item_ids])\n",
    "        r_attrs_and_values = [(clf._column_id_to_name[c], v) for c, v in r_attr_ixs_and_values]\n",
    "        return {'body': r_attrs_and_values, 'class': r_class}\n",
    "\n",
    "    rules = []\n",
    "    \n",
    "    for r in encoded_rules:\n",
    "        # For each of its attributes and values\n",
    "        for a,v in decode_rule(r, clf)['body']:\n",
    "            # If rule uses an attribute's value different from the instance's\n",
    "            if decoded_instance[a] != v:\n",
    "                # Exit the inner loop, not entering the else clause, therefore not adding the rule\n",
    "                break\n",
    "        # https://docs.python.org/3/tutorial/controlflow.html#break-and-continue-statements-and-else-clauses-on-loops\n",
    "        else:\n",
    "            # If the inner loop has completed normally without break-ing, then all of the rule's\n",
    "            # attribute values are in the instance as well, so we will use this rule\n",
    "\n",
    "            # Get the instance attribute index from the rule's item_ids\n",
    "            di = decoded_instance.index\n",
    "            \n",
    "            # Class matching\n",
    "            if decode_rule(r, clf)['class'] == decoded_instance.iloc[-1]:\n",
    "                rules.append(list(sorted([di.get_loc(a) + 1 for a, v in decode_rule(r, clf)['body']])))\n",
    "    \n",
    "    return rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess  \n",
    "import os\n",
    "from shutil import rmtree\n",
    "import tempfile\n",
    "\n",
    "def AL3(instance_ix, train):\n",
    "    with tempfile.TemporaryDirectory() as d:\n",
    "        with open(join(d, 'Knnres.arff'), \"w\") as knnres:\n",
    "            arff.dump(\n",
    "                train.to_arff_obj(),\n",
    "                knnres)\n",
    "\n",
    "        with open(join(d, 'Filetest.arff'), \"w\") as filetest:\n",
    "            arff.dump(\n",
    "                Dataset(\n",
    "                    [train.inverse_transform_instance(train[instance_ix])],\n",
    "                    train.columns\n",
    "                ).to_arff_obj(),\n",
    "                filetest)\n",
    "\n",
    "        subprocess.call(['java', '-jar', 'AL3.jar',\n",
    "                         '-no-cv',\n",
    "                         '-t', knnres.name,\n",
    "                         '-T', filetest.name,\n",
    "                         '-S', '1.0', # minimum support\n",
    "                         '-C', '50.0', # minimum confidence\n",
    "                         '-PN', d, # temporary files folder\n",
    "                         '-SP', '10', # measure threshold\n",
    "                         '-NRUL','1']) #  maximum number of rules to classify a transaction\n",
    "\n",
    "        with open(join(d, 'impo_rules.txt'), \"r\") as impo_rules:\n",
    "            rules = impo_rules.readlines()\n",
    "\n",
    "        def parse_rules(rules_lines):\n",
    "            union_rule = []\n",
    "            rules = []\n",
    "\n",
    "            for rule_line in rules_lines:\n",
    "                rule = []\n",
    "\n",
    "                for attribute_str in rule_line.split(\",\"):\n",
    "                    attribute = int(attribute_str)\n",
    "                    rule.append(attribute)\n",
    "                    union_rule.append(attribute)\n",
    "\n",
    "                rules.append(rule)\n",
    "\n",
    "            # Remove duplicates\n",
    "            union_rule = list(sorted(set(union_rule)))\n",
    "\n",
    "            if union_rule not in rules:\n",
    "                rules.append(union_rule)\n",
    "\n",
    "            return rules\n",
    "\n",
    "        return parse_rules(rules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(join(DEFAULT_DIR, \"datasets/monks.arff\")) as monks_f:\n",
    "    monks_train, monks_explain, monks_explain_indices = import_dataset_arff(monks_f, [], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[[1, 2]]\n",
      "[[1, 2]]\n",
      "1\n",
      "[[1, 2]]\n",
      "[[1, 2]]\n",
      "2\n",
      "[[1, 2]]\n",
      "[[1, 2]]\n",
      "3\n",
      "[[1, 2]]\n",
      "[[1, 2]]\n",
      "4\n",
      "[[1, 2]]\n",
      "[[1, 2]]\n",
      "5\n",
      "[[1, 2]]\n",
      "[[1, 2]]\n",
      "6\n",
      "[[1, 2]]\n",
      "[[1, 2]]\n",
      "7\n",
      "[[1, 2]]\n",
      "[[5], [1, 2]]\n",
      "8\n",
      "[[1, 2]]\n",
      "[[1, 2]]\n",
      "9\n",
      "[[1, 2]]\n",
      "[[1, 2, 5], [1, 2]]\n",
      "10\n",
      "[[1, 2]]\n",
      "[[1, 2, 6], [1, 2]]\n",
      "11\n",
      "[[1, 2]]\n",
      "[[5]]\n",
      "12\n",
      "[[1, 2]]\n",
      "[[1, 2, 5], [1, 2]]\n",
      "13\n",
      "[[1, 2]]\n",
      "[[1, 2, 6], [1, 2]]\n",
      "14\n",
      "[[1, 2]]\n",
      "[[1, 2, 6], [1, 2]]\n",
      "15\n",
      "[[1, 2]]\n",
      "[[1, 2, 6], [1, 2, 3], [1, 2], [3, 5, 6]]\n",
      "16\n",
      "[[1, 2]]\n",
      "[[1, 2, 5], [1, 2, 6], [1, 2, 3], [1, 2]]\n",
      "17\n",
      "[[1, 2]]\n",
      "[[1, 2, 3], [1, 2]]\n",
      "18\n",
      "[[1, 2]]\n",
      "[[1, 2, 6], [1, 2, 3], [1, 2], [3, 5, 6]]\n",
      "19\n",
      "[[1, 2]]\n",
      "[[1, 2, 5], [1, 2, 3], [1, 2]]\n",
      "20\n",
      "[[1, 2]]\n",
      "[[1, 2, 5], [1, 2, 6], [1, 2, 3], [1, 2]]\n",
      "21\n",
      "[[1, 2]]\n",
      "[[1, 2], [3, 5, 6]]\n",
      "22\n",
      "[[1, 2]]\n",
      "[[1, 2]]\n",
      "23\n",
      "[[1, 2]]\n",
      "[[5]]\n",
      "24\n",
      "[[1, 2]]\n",
      "[[1, 2]]\n",
      "25\n",
      "[[1, 2]]\n",
      "[[1, 2], [3, 5, 6]]\n",
      "26\n",
      "[[1, 2]]\n",
      "[[1, 2, 6], [1, 2]]\n",
      "27\n",
      "[[1, 2]]\n",
      "[[1, 2, 6], [1, 2, 3], [1, 2], [3, 5, 6]]\n",
      "28\n",
      "[[1, 2]]\n",
      "[[5]]\n",
      "29\n",
      "[[1, 2]]\n",
      "[[1, 2, 6], [1, 2, 3], [1, 2], [3, 5, 6]]\n",
      "30\n",
      "[[1, 2]]\n",
      "[[1, 2, 6], [1, 2, 3], [1, 2]]\n",
      "31\n",
      "[[1, 2]]\n",
      "[[1, 2, 3], [1, 2]]\n",
      "32\n",
      "[[1, 2]]\n",
      "[[1, 2, 6], [1, 2, 3], [1, 2]]\n",
      "33\n",
      "[[1, 2]]\n",
      "[[5]]\n",
      "34\n",
      "[[1, 2]]\n",
      "[[1, 2, 3], [1, 2]]\n",
      "35\n",
      "[[1, 2]]\n",
      "[[1, 2, 3], [1, 2]]\n",
      "36\n",
      "[[1, 2]]\n",
      "[[1, 2]]\n",
      "37\n",
      "[[1, 2]]\n",
      "[[1, 2, 6], [1, 2]]\n",
      "38\n",
      "[[1, 2]]\n",
      "[[5]]\n",
      "39\n",
      "[[1, 2]]\n",
      "[[5]]\n",
      "40\n",
      "[[1, 2]]\n",
      "[[1, 2, 6], [1, 2, 4], [1, 2]]\n",
      "41\n",
      "[[1, 2]]\n",
      "[[1, 2, 4], [1, 2]]\n",
      "42\n",
      "[[1, 2]]\n",
      "[[1, 2, 5], [1, 2, 4], [1, 2], [3, 5, 6]]\n",
      "43\n",
      "[[1, 2]]\n",
      "[[1, 2, 5], [1, 2, 6], [1, 2, 4], [1, 2]]\n",
      "44\n",
      "[[1, 2]]\n",
      "[[1, 2, 5], [1, 2], [3, 5, 6]]\n",
      "45\n",
      "[[1, 2]]\n",
      "[[1, 2, 3], [1, 2, 6], [1, 2], [3, 5, 6]]\n",
      "46\n",
      "[[1, 2]]\n",
      "[[1, 2, 5], [1, 2, 3], [1, 2, 6], [1, 2]]\n",
      "47\n",
      "[[1, 2]]\n",
      "[[1, 2, 3], [1, 2, 4], [1, 2]]\n",
      "48\n",
      "[[1, 2]]\n",
      "[[1, 2, 5], [1, 2, 3], [1, 2, 6], [1, 2, 4], [1, 2]]\n",
      "49\n",
      "[[1, 2]]\n",
      "[[1, 2, 3], [1, 2, 6], [1, 2], [3, 5, 6]]\n"
     ]
    }
   ],
   "source": [
    "for i in range(min(len(monks_train),50)):\n",
    "    print(i)\n",
    "    a = AL3(i, monks_train)\n",
    "    b = l3wrapper(i, monks_train)\n",
    "    print(a)\n",
    "    print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sopra: run con `0.075`\n",
    "\n",
    "Sotto: run con `0.01`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[[1, 2]]\n",
      "[[1, 2]]\n",
      "1\n",
      "[[1, 2]]\n",
      "[[1, 2]]\n",
      "2\n",
      "[[1, 2]]\n",
      "[[1, 2]]\n",
      "3\n",
      "[[1, 2]]\n",
      "[[1, 2]]\n",
      "4\n",
      "[[1, 2]]\n",
      "[[1, 2]]\n",
      "5\n",
      "[[1, 2]]\n",
      "[[1, 2]]\n",
      "6\n",
      "[[1, 2]]\n",
      "[[1, 2]]\n",
      "7\n",
      "[[1, 2]]\n",
      "[[5], [1, 2]]\n",
      "8\n",
      "[[1, 2]]\n",
      "[[1, 2, 5]]\n",
      "9\n",
      "[[1, 2]]\n",
      "[[1, 2, 5]]\n",
      "10\n",
      "[[1, 2]]\n",
      "[[1, 2, 5]]\n",
      "11\n",
      "[[1, 2]]\n",
      "[[5]]\n",
      "12\n",
      "[[1, 2]]\n",
      "[[1, 2, 5]]\n",
      "13\n",
      "[[1, 2]]\n",
      "[[1, 2, 5]]\n",
      "14\n",
      "[[1, 2]]\n",
      "[[1, 2, 5]]\n",
      "15\n",
      "[[1, 2]]\n",
      "[[1, 2, 5]]\n",
      "16\n",
      "[[1, 2]]\n",
      "[[1, 2, 5]]\n",
      "17\n",
      "[[1, 2]]\n",
      "[[1, 2, 5]]\n",
      "18\n",
      "[[1, 2]]\n",
      "[[1, 2, 5]]\n",
      "19\n",
      "[[1, 2]]\n",
      "[[1, 2, 5]]\n",
      "20\n",
      "[[1, 2]]\n",
      "[[1, 2, 5]]\n",
      "21\n",
      "[[1, 2]]\n",
      "[[1, 2, 5]]\n",
      "22\n",
      "[[1, 2]]\n",
      "[[1, 2, 5]]\n",
      "23\n",
      "[[1, 2]]\n",
      "[[5]]\n",
      "24\n",
      "[[1, 2]]\n",
      "[[1, 2, 5]]\n",
      "25\n",
      "[[1, 2]]\n",
      "[[1, 2, 5]]\n",
      "26\n",
      "[[1, 2]]\n",
      "[[1, 2, 5]]\n",
      "27\n",
      "[[1, 2]]\n",
      "[[1, 2, 5]]\n",
      "28\n",
      "[[1, 2]]\n",
      "[[5]]\n",
      "29\n",
      "[[1, 2]]\n",
      "[[1, 2, 5]]\n",
      "30\n",
      "[[1, 2]]\n",
      "[[1, 2, 5]]\n",
      "31\n",
      "[[1, 2]]\n",
      "[[1, 2, 5]]\n",
      "32\n",
      "[[1, 2]]\n",
      "[[1, 2, 5]]\n",
      "33\n",
      "[[1, 2]]\n",
      "[[5]]\n",
      "34\n",
      "[[1, 2]]\n",
      "[[1, 2, 5]]\n",
      "35\n",
      "[[1, 2]]\n",
      "[[1, 2, 5]]\n",
      "36\n",
      "[[1, 2]]\n",
      "[[1, 2, 5]]\n",
      "37\n",
      "[[1, 2]]\n",
      "[[1, 2, 5]]\n",
      "38\n",
      "[[1, 2]]\n",
      "[[5]]\n",
      "39\n",
      "[[1, 2]]\n",
      "[[5]]\n",
      "40\n",
      "[[1, 2]]\n",
      "[[1, 2, 5]]\n",
      "41\n",
      "[[1, 2]]\n",
      "[[1, 2, 5]]\n",
      "42\n",
      "[[1, 2]]\n",
      "[[1, 2, 5]]\n",
      "43\n",
      "[[1, 2]]\n",
      "[[1, 2, 5]]\n",
      "44\n",
      "[[1, 2]]\n",
      "[[1, 2, 5]]\n",
      "45\n",
      "[[1, 2]]\n",
      "[[1, 2, 5]]\n",
      "46\n",
      "[[1, 2]]\n",
      "[[1, 2, 5]]\n",
      "47\n",
      "[[1, 2]]\n",
      "[[1, 2, 5]]\n",
      "48\n",
      "[[1, 2]]\n",
      "[[1, 2, 5]]\n",
      "49\n",
      "[[1, 2]]\n",
      "[[1, 2, 5]]\n"
     ]
    }
   ],
   "source": [
    "for i in range(min(len(monks_train),50)):\n",
    "    print(i)\n",
    "    a = AL3(i, monks_train)\n",
    "    b = l3wrapper(i, monks_train)\n",
    "    print(a)\n",
    "    print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zoo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(join(DEFAULT_DIR, \"datasets/zoo.arff\")) as zoo_f:\n",
    "    zoo_train, zoo_explain, zoo_explain_indices = import_dataset_arff(zoo_f, [], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "1\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "2\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "[[1, 2, 3, 4, 5, 6, 8, 9, 10, 12, 13, 14]]\n",
      "3\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "4\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "5\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "6\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "[[1, 2, 3, 4, 5, 6, 8, 9, 10, 12, 13, 14]]\n",
      "7\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "8\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "9\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "[[1, 2, 3, 4, 8, 9, 10, 11, 12, 13, 14]]\n",
      "10\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "[[1, 2, 3, 4, 5, 7, 8, 9, 10, 11, 12, 14, 15], [1, 2, 3, 4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16]]\n",
      "11\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "[[1, 2, 3, 4, 5, 7, 8, 9, 10, 11, 12, 14, 15]]\n",
      "12\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "[[1, 2, 3, 4, 8, 9, 10, 11, 12, 13, 14]]\n",
      "13\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "[[1, 2, 3, 4, 5, 6, 8, 9, 10, 12, 13, 14]]\n",
      "14\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "15\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "[[1, 2, 3, 4, 8, 9, 10, 11, 12, 13, 14]]\n",
      "16\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "[[1, 2, 3, 4, 8, 9, 10, 11, 12, 13, 14]]\n",
      "17\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "18\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "[[1, 2, 3, 4, 8, 9, 10, 11, 12, 13, 14]]\n",
      "19\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "[[2, 3, 4, 6, 8, 9, 10, 12, 13, 14, 16]]\n",
      "20\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "[[1, 2, 3, 4, 5, 6, 8, 9, 10, 11, 12, 13, 15, 16]]\n",
      "21\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "22\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "23\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "24\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "[[2, 3, 4, 6, 8, 9, 10, 12, 13, 14, 16]]\n",
      "25\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "26\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "[[1, 2, 3, 4, 8, 9, 10, 11, 12, 13, 14]]\n",
      "27\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "[[1, 2, 3, 4, 5, 6, 8, 9, 10, 12, 13, 14]]\n",
      "28\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "29\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "30\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "[[1, 2, 3, 4, 8, 9, 10, 11, 12, 13, 14]]\n",
      "31\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "[[1, 2, 3, 4, 5, 6, 8, 9, 10, 12, 13, 14]]\n",
      "32\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "[[2, 3, 4, 6, 8, 9, 10, 12, 13, 14, 16]]\n",
      "33\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "[[2, 3, 4, 6, 8, 9, 10, 12, 13, 14, 16]]\n",
      "34\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "[[1, 2, 3, 4, 8, 9, 10, 11, 12, 13, 14]]\n",
      "35\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "[[2, 3, 4, 6, 8, 9, 10, 12, 13, 14, 16]]\n",
      "36\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "[[1, 2, 3, 4, 8, 9, 10, 11, 12, 13, 14]]\n",
      "37\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "38\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "39\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "[[1, 2, 3, 4, 5, 7, 8, 9, 10, 11, 12, 14, 15]]\n",
      "40\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "41\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "42\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "[[2, 3, 4, 6, 8, 9, 10, 12, 13, 14, 16]]\n",
      "43\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "[[1, 2, 3, 4, 5, 6, 8, 9, 10, 11, 12, 13, 15, 16]]\n",
      "44\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "[[1, 2, 3, 4, 5, 7, 8, 9, 10, 11, 12, 14, 15]]\n",
      "45\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "46\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "[[1, 2, 3, 4, 8, 9, 10, 11, 12, 13, 14]]\n",
      "47\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "[[1, 2, 3, 4, 8, 9, 10, 11, 12, 13, 14]]\n",
      "48\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "[[1, 2, 3, 4, 8, 9, 10, 11, 12, 13, 14]]\n",
      "49\n",
      "[[2, 3, 4, 8, 9, 10, 11]]\n",
      "[[1, 2, 3, 4, 5, 6, 8, 9, 10, 12, 13, 14]]\n"
     ]
    }
   ],
   "source": [
    "for i in range(min(len(zoo_train), 50)):\n",
    "    print(i)\n",
    "    a = AL3(i, zoo_train)\n",
    "    b = l3wrapper(i, zoo_train)\n",
    "    print(a)\n",
    "    print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(join(DEFAULT_DIR, \"datasets/adult_d.arff\")) as adult_f:\n",
    "    with open(join(DEFAULT_DIR, \"datasets/adult_d_explain.arff\")) as adult_explain_f:\n",
    "        adult_train, adult_explain, adult_explain_indices = import_datasets_arff(adult_f, adult_explain_f, [], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(min(len(adult_train), 50)):\n",
    "    print(i)\n",
    "    a = AL3(i, adult_train)\n",
    "    print(a)\n",
    "    b = l3wrapper(i, adult_train)\n",
    "    print(b)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "299px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 752.85,
   "position": {
    "height": "783.85px",
    "left": "1531px",
    "right": "20px",
    "top": "117px",
    "width": "374px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
