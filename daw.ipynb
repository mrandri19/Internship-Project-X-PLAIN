{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare `l3wrapper` and `AL3.jar`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/andrea/Documents/Politecnico/tirocinio/src/../\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from os.path import join\n",
    "from typing import Tuple, List\n",
    "\n",
    "import arff\n",
    "from snapshottest import TestCase\n",
    "\n",
    "from src import DEFAULT_DIR\n",
    "from src.XPLAIN_explainer import XPLAIN_explainer\n",
    "from src.dataset import Dataset\n",
    "\n",
    "\n",
    "def load_arff(f) -> Dataset:\n",
    "    a = arff.load(f)\n",
    "    dataset = Dataset(a['data'], a['attributes'])\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def import_dataset_arff(f, explain_indices: List[int],\n",
    "                        random_explain_dataset: bool) -> Tuple[Dataset, Dataset, List[str]]:\n",
    "    dataset = load_arff(f)\n",
    "\n",
    "    dataset_len = len(dataset)\n",
    "    train_indices = list(range(dataset_len))\n",
    "\n",
    "    if random_explain_dataset:\n",
    "        random.seed(1)\n",
    "        # small dataset\n",
    "        MAX_SAMPLE_COUNT = 100\n",
    "        if dataset_len < (2 * MAX_SAMPLE_COUNT):\n",
    "            samples = int(0.2 * dataset_len)\n",
    "        else:\n",
    "            samples = MAX_SAMPLE_COUNT\n",
    "\n",
    "        # Randomly pick some instances to remove from the training dataset and use in the\n",
    "        # explain dataset\n",
    "        explain_indices = list(random.sample(train_indices, samples))\n",
    "    for i in explain_indices:\n",
    "        train_indices.remove(i)\n",
    "\n",
    "    train_dataset = Dataset.from_indices(train_indices, dataset)\n",
    "    explain_dataset = Dataset.from_indices(explain_indices, dataset)\n",
    "\n",
    "    return train_dataset, explain_dataset, [str(i) for i in explain_indices]\n",
    "\n",
    "\n",
    "def import_datasets_arff(f, f_explain, explain_indices: List[int],\n",
    "                         random_explain_dataset: bool) -> Tuple[Dataset, Dataset, List[str]]:\n",
    "    train_dataset = load_arff(f)\n",
    "    explain_dataset = load_arff(f_explain)\n",
    "\n",
    "    len_explain_dataset = len(explain_dataset)\n",
    "\n",
    "    if random_explain_dataset:\n",
    "        random.seed(7)\n",
    "        explain_indices = list(random.sample(range(len_explain_dataset), 300))\n",
    "        explain_dataset = Dataset.from_indices(explain_indices, explain_dataset)\n",
    "\n",
    "    return train_dataset, explain_dataset, [str(i) for i in explain_indices]\n",
    "\n",
    "\n",
    "def get_classifier(classifier_name: str):\n",
    "    if classifier_name == \"sklearn_nb\":\n",
    "        from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "        skl_clf = MultinomialNB()\n",
    "\n",
    "        return skl_clf\n",
    "\n",
    "    elif classifier_name == \"sklearn_rf\":\n",
    "        from sklearn.ensemble import RandomForestClassifier\n",
    "        from sklearn.pipeline import make_pipeline\n",
    "        from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "        pipe = make_pipeline(OneHotEncoder(), RandomForestClassifier(random_state=42))\n",
    "        skl_clf = pipe\n",
    "\n",
    "        return skl_clf\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Classifier not available\")\n",
    "\n",
    "\n",
    "def get_explanation(dataset_name: str, classifier_name: str):\n",
    "    explain_dataset_indices = []\n",
    "    if dataset_name in [join(DEFAULT_DIR, \"datasets/adult_d.arff\"),\n",
    "                        join(DEFAULT_DIR, \"datasets/compas-scores-two-years_d.arff\")]:\n",
    "        with open(dataset_name) as f, open(dataset_name[:-5] + \"_explain.arff\") as f_explain:\n",
    "            train_dataset, explain_dataset, explain_indices = import_datasets_arff(f, f_explain,\n",
    "                                                                                   explain_dataset_indices,\n",
    "                                                                                   True)\n",
    "    else:\n",
    "        with open(dataset_name) as f:\n",
    "            train_dataset, explain_dataset, explain_indices = import_dataset_arff(\n",
    "                f, explain_dataset_indices, True)\n",
    "\n",
    "    clf = get_classifier(classifier_name).fit(train_dataset.X_numpy(),\n",
    "                                              train_dataset.Y_numpy())\n",
    "    explainer = XPLAIN_explainer(clf, train_dataset)\n",
    "\n",
    "    instance = explain_dataset.get_decoded(0)\n",
    "\n",
    "    cc = explain_dataset.class_column_name()\n",
    "    target_class_index = instance[cc]\n",
    "\n",
    "    return explainer.explain_instance(explain_dataset[0], target_class_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l3wrapper(instance_ix, train):\n",
    "    from l3wrapper.l3wrapper import L3Classifier\n",
    "    \n",
    "    # TODO: Cache\n",
    "    clf = L3Classifier(min_sup=0.01, min_conf=0.50)\n",
    "    clf.fit(train.X_decoded(),\n",
    "            train.Y_decoded(),\n",
    "            column_names=train.X_decoded().columns.to_list())\n",
    "    \n",
    "    decoded_instance = train.inverse_transform_instance(train[instance_ix])\n",
    "    encoded_rules = clf.lvl1_rules_\n",
    "\n",
    "    def decode_rule(r, clf):\n",
    "        r_class = clf._class_dict[r.class_id]\n",
    "        r_attr_ixs_and_values = sorted([clf._item_id_to_item[i] for i in r.item_ids])\n",
    "        r_attrs_and_values = [(clf._column_id_to_name[c], v) for c, v in r_attr_ixs_and_values]\n",
    "        return {'body': r_attrs_and_values, 'class': r_class}\n",
    "\n",
    "    rules = []\n",
    "   \n",
    "    # TODO: CHECK BENE\n",
    "    for r in encoded_rules:\n",
    "        # For each of its attributes and values\n",
    "        for a,v in decode_rule(r, clf)['body']:\n",
    "            # If rule uses an attribute's value different from the instance's\n",
    "            if decoded_instance[a] != v:\n",
    "                # Exit the inner loop, not entering the else clause, therefore not adding the rule\n",
    "                break\n",
    "        # https://docs.python.org/3/tutorial/controlflow.html#break-and-continue-statements-and-else-clauses-on-loops\n",
    "        else:\n",
    "            # If the inner loop has completed normally without break-ing, then all of the rule's\n",
    "            # attribute values are in the instance as well, so we will use this rule\n",
    "\n",
    "            # Get the instance attribute index from the rule's item_ids\n",
    "            di = decoded_instance.index\n",
    "            \n",
    "            # Class matching\n",
    "            if decode_rule(r, clf)['class'] == decoded_instance.iloc[-1]:\n",
    "                rules.append(list(sorted([di.get_loc(a) + 1 for a, v in decode_rule(r, clf)['body']])))\n",
    "    \n",
    "    return rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess  \n",
    "import os\n",
    "from shutil import rmtree\n",
    "import tempfile\n",
    "\n",
    "def AL3(instance_ix, train):\n",
    "    with tempfile.TemporaryDirectory() as d:\n",
    "        with open(join(d, 'Knnres.arff'), \"w\") as knnres:\n",
    "            arff.dump(\n",
    "                train.to_arff_obj(),\n",
    "                knnres)\n",
    "\n",
    "        with open(join(d, 'Filetest.arff'), \"w\") as filetest:\n",
    "            arff.dump(\n",
    "                Dataset(\n",
    "                    [train.inverse_transform_instance(train[instance_ix])],\n",
    "                    train.columns\n",
    "                ).to_arff_obj(),\n",
    "                filetest)\n",
    "\n",
    "        subprocess.call(['java', '-jar', 'AL3.jar',\n",
    "                         '-no-cv',\n",
    "                         '-t', knnres.name,\n",
    "                         '-T', filetest.name,\n",
    "                         '-S', '1.0', # minimum support\n",
    "                         '-C', '50.0', # minimum confidence\n",
    "                         '-PN', d, # temporary files folder\n",
    "                         '-SP', '10', # measure threshold\n",
    "                         '-NRUL','1']) #  maximum number of rules to classify a transaction\n",
    "\n",
    "        with open(join(d, 'impo_rules.txt'), \"r\") as impo_rules:\n",
    "            rules = impo_rules.readlines()\n",
    "\n",
    "        def parse_rules(rules_lines):\n",
    "            union_rule = []\n",
    "            rules = []\n",
    "\n",
    "            for rule_line in rules_lines:\n",
    "                rule = []\n",
    "\n",
    "                for attribute_str in rule_line.split(\",\"):\n",
    "                    attribute = int(attribute_str)\n",
    "                    rule.append(attribute)\n",
    "                    union_rule.append(attribute)\n",
    "\n",
    "                rules.append(rule)\n",
    "\n",
    "            # Remove duplicates\n",
    "            union_rule = list(sorted(set(union_rule)))\n",
    "\n",
    "            if union_rule not in rules:\n",
    "                rules.append(union_rule)\n",
    "\n",
    "            return rules\n",
    "\n",
    "        return parse_rules(rules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(join(DEFAULT_DIR, \"datasets/monks.arff\")) as monks_f:\n",
    "    monks_train, monks_explain, monks_explain_indices = import_dataset_arff(monks_f, [], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[[1, 2]]\n",
      "[[1, 2]]\n",
      "1\n",
      "[[1, 2]]\n",
      "[[1, 2]]\n",
      "2\n",
      "[[1, 2]]\n",
      "[[1, 2]]\n",
      "3\n",
      "[[1, 2]]\n",
      "[[1, 2]]\n",
      "4\n",
      "[[1, 2]]\n",
      "[[1, 2]]\n",
      "5\n",
      "[[1, 2]]\n",
      "[[1, 2]]\n",
      "6\n",
      "[[1, 2]]\n",
      "[[1, 2]]\n",
      "7\n",
      "[[1, 2]]\n",
      "[[5], [1, 2]]\n",
      "8\n",
      "[[1, 2]]\n",
      "[[1, 2, 5]]\n",
      "9\n",
      "[[1, 2]]\n",
      "[[1, 2, 5]]\n",
      "10\n",
      "[[1, 2]]\n",
      "[[1, 2, 5]]\n",
      "11\n",
      "[[1, 2]]\n",
      "[[5]]\n",
      "12\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-1f4c7536748d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonks_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAL3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonks_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ml3wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonks_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-62ab069e685b>\u001b[0m in \u001b[0;36mAL3\u001b[0;34m(instance_ix, train)\u001b[0m\n\u001b[1;32m     27\u001b[0m                          \u001b[0;34m'-PN'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# temporary files folder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m                          \u001b[0;34m'-SP'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'10'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# measure threshold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m                          '-NRUL','1']) #  maximum number of rules to classify a transaction\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'impo_rules.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mimpo_rules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/subprocess.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(timeout, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    287\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpopenargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m             \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/subprocess.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout, endtime)\u001b[0m\n\u001b[1;32m   1475\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1476\u001b[0m                             \u001b[0;32mbreak\u001b[0m  \u001b[0;31m# Another thread waited.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1477\u001b[0;31m                         \u001b[0;34m(\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1478\u001b[0m                         \u001b[0;31m# Check the pid and loop as waitpid has been known to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1479\u001b[0m                         \u001b[0;31m# return 0 even without WNOHANG in odd situations.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/subprocess.py\u001b[0m in \u001b[0;36m_try_wait\u001b[0;34m(self, wait_flags)\u001b[0m\n\u001b[1;32m   1422\u001b[0m             \u001b[0;34m\"\"\"All callers to this function MUST hold self._waitpid_lock.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1423\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1424\u001b[0;31m                 \u001b[0;34m(\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwaitpid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait_flags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1425\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mChildProcessError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1426\u001b[0m                 \u001b[0;31m# This happens if SIGCLD is set to be ignored or waiting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(min(len(monks_train),50)):\n",
    "    print(i)\n",
    "    a = AL3(i, monks_train)\n",
    "    b = l3wrapper(i, monks_train)\n",
    "    print(a)\n",
    "    print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analizziamo l'istanza 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "a    1\n",
       "b    1\n",
       "c    2\n",
       "d    3\n",
       "e    1\n",
       "f    2\n",
       "y    1\n",
       "Name: 8, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monks_train.get_decoded(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[5], [1, 2]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l3wrapper(7, monks_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AL3(7, monks_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoded instance\n",
      "a    1\n",
      "b    1\n",
      "c    2\n",
      "d    3\n",
      "e    1\n",
      "f    2\n",
      "y    1\n",
      "dtype: object\n",
      "\n",
      "Rule(id:0;item_ids:13;sup:111;conf:100.0) ✔ matched\n",
      "Rule(id:3;item_ids:1,2;sup:51;conf:100.0) ✔ matched\n",
      "\n",
      "Rule(id:0;item_ids:13;sup:111;conf:100.0)\t{'body': [('e', '1')], 'class': '1'}\n",
      "Rule(id:1;item_ids:17,15;sup:55;conf:100.0)\t{'body': [('a', '3'), ('b', '3')], 'class': '1'}\n",
      "Rule(id:2;item_ids:16,14;sup:54;conf:100.0)\t{'body': [('a', '2'), ('b', '2')], 'class': '1'}\n",
      "Rule(id:3;item_ids:1,2;sup:51;conf:100.0)\t{'body': [('a', '1'), ('b', '1')], 'class': '1'}\n",
      "Rule(id:4;item_ids:16,2,12;sup:17;conf:100.0)\t{'body': [('a', '2'), ('b', '1'), ('e', '4')], 'class': '0'}\n",
      "Rule(id:5;item_ids:1,5,14;sup:16;conf:100.0)\t{'body': [('a', '1'), ('b', '2'), ('e', '3')], 'class': '0'}\n",
      "Rule(id:6;item_ids:1,12,14;sup:15;conf:100.0)\t{'body': [('a', '1'), ('b', '2'), ('e', '4')], 'class': '0'}\n",
      "Rule(id:7;item_ids:17,2,10;sup:15;conf:100.0)\t{'body': [('a', '3'), ('b', '1'), ('e', '2')], 'class': '0'}\n",
      "Rule(id:8;item_ids:1,10,15;sup:14;conf:100.0)\t{'body': [('a', '1'), ('b', '3'), ('e', '2')], 'class': '0'}\n",
      "Rule(id:9;item_ids:16,2,10;sup:14;conf:100.0)\t{'body': [('a', '2'), ('b', '1'), ('e', '2')], 'class': '0'}\n",
      "Rule(id:10;item_ids:16,5,15;sup:14;conf:100.0)\t{'body': [('a', '2'), ('b', '3'), ('e', '3')], 'class': '0'}\n",
      "Rule(id:11;item_ids:16,12,15;sup:13;conf:100.0)\t{'body': [('a', '2'), ('b', '3'), ('e', '4')], 'class': '0'}\n",
      "Rule(id:12;item_ids:16,2,5;sup:13;conf:100.0)\t{'body': [('a', '2'), ('b', '1'), ('e', '3')], 'class': '0'}\n",
      "Rule(id:13;item_ids:1,12,15;sup:12;conf:100.0)\t{'body': [('a', '1'), ('b', '3'), ('e', '4')], 'class': '0'}\n",
      "Rule(id:14;item_ids:1,5,15;sup:12;conf:100.0)\t{'body': [('a', '1'), ('b', '3'), ('e', '3')], 'class': '0'}\n",
      "Rule(id:15;item_ids:17,10,14;sup:12;conf:100.0)\t{'body': [('a', '3'), ('b', '2'), ('e', '2')], 'class': '0'}\n",
      "Rule(id:16;item_ids:1,10,14;sup:11;conf:100.0)\t{'body': [('a', '1'), ('b', '2'), ('e', '2')], 'class': '0'}\n",
      "Rule(id:17;item_ids:17,5,14;sup:11;conf:100.0)\t{'body': [('a', '3'), ('b', '2'), ('e', '3')], 'class': '0'}\n",
      "Rule(id:18;item_ids:17,12,14;sup:10;conf:100.0)\t{'body': [('a', '3'), ('b', '2'), ('e', '4')], 'class': '0'}\n",
      "Rule(id:19;item_ids:16,10,15;sup:9;conf:100.0)\t{'body': [('a', '2'), ('b', '3'), ('e', '2')], 'class': '0'}\n",
      "Rule(id:20;item_ids:17,2,12;sup:8;conf:100.0)\t{'body': [('a', '3'), ('b', '1'), ('e', '4')], 'class': '0'}\n",
      "Rule(id:21;item_ids:17,2,5;sup:8;conf:100.0)\t{'body': [('a', '3'), ('b', '1'), ('e', '3')], 'class': '0'}\n"
     ]
    }
   ],
   "source": [
    "def test(instance_ix, train):\n",
    "    from l3wrapper.l3wrapper import L3Classifier\n",
    "    \n",
    "    # TODO: Cache\n",
    "    clf = L3Classifier(min_sup=0.01, min_conf=0.50)\n",
    "    clf.fit(train.X_decoded(),\n",
    "            train.Y_decoded(),\n",
    "            column_names=train.X_decoded().columns.to_list())\n",
    "    \n",
    "    decoded_instance = train.inverse_transform_instance(train[instance_ix])\n",
    "    encoded_rules = clf.lvl1_rules_\n",
    "\n",
    "    def decode_rule(r, clf):\n",
    "        r_class = clf._class_dict[r.class_id]\n",
    "        r_attr_ixs_and_values = sorted([clf._item_id_to_item[i] for i in r.item_ids])\n",
    "        r_attrs_and_values = [(clf._column_id_to_name[c], v) for c, v in r_attr_ixs_and_values]\n",
    "        return {'body': r_attrs_and_values, 'class': r_class}\n",
    "\n",
    "    rules = []\n",
    "   \n",
    "    for r in encoded_rules:\n",
    "        # For each of its attributes and values\n",
    "        for a,v in decode_rule(r, clf)['body']:\n",
    "            # If rule uses an attribute's value different from the instance's\n",
    "            if decoded_instance[a] != v:\n",
    "                # Exit the inner loop, not entering the else clause, therefore not adding the rule\n",
    "                break\n",
    "        # https://docs.python.org/3/tutorial/controlflow.html#break-and-continue-statements-and-else-clauses-on-loops\n",
    "        else:\n",
    "            # If the inner loop has completed normally without break-ing, then all of the rule's\n",
    "            # attribute values are in the instance as well, so we will use this rule\n",
    "\n",
    "            # Get the instance attribute index from the rule's item_ids\n",
    "            di = decoded_instance.index\n",
    "            \n",
    "            # Class matching\n",
    "            if decode_rule(r, clf)['class'] == decoded_instance.iloc[-1]:\n",
    "                print(r,'✔ matched')\n",
    "                rules.append(list(sorted([di.get_loc(a) + 1 for a, v in decode_rule(r, clf)['body']])))\n",
    "    print()\n",
    "    return clf, rules\n",
    "\n",
    "\n",
    "print(f'decoded instance\\n{monks_train.inverse_transform_instance(monks_train[7])}\\n')\n",
    "c, _ = test(7, monks_train)\n",
    "for rule in c.lvl1_rules_:\n",
    "    def decode_rule(r, clf):\n",
    "        r_class = clf._class_dict[r.class_id]\n",
    "        r_attr_ixs_and_values = sorted([clf._item_id_to_item[i] for i in r.item_ids])\n",
    "        r_attrs_and_values = [(clf._column_id_to_name[c], v) for c, v in r_attr_ixs_and_values]\n",
    "        return {'body': r_attrs_and_values, 'class': r_class}\n",
    "    print(f\"{rule}\\t{decode_rule(rule, c)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 752.85,
   "position": {
    "height": "783.85px",
    "left": "1535px",
    "right": "20px",
    "top": "118px",
    "width": "374px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
